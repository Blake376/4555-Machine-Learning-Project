1.
For our data, we used a large collection of wine reviews (https://www.kaggle.com/zynicide/wine-reviews/data). The original data set was comprised of several columns, including the wineâ€™s country of production, variety, price, critic score, and an arbitrary numbering. Before using the data in our algorithm, we cut out all columns except for the score, price, and wine variety. The original scores ranged from 80 to 100. In different instances of the algorithm, we put these scores into different numbers of classes. The first instance uses two classes, 0 if the score was 90 or below, 1 if score was above 90. The second instance uses three classes, 0 if the score was below 88, 1 if the score was between 88 and 94 (including 88 and 94), 2 if the score  was above 94. The third instance uses four classes, 0 if the score was below 85, 1 if the score was between 85 and 89 (including 85 and 89), 2 if the score was between 90 and 94 (including 90 and 94), 3 if the score was above 94. We used these score classes as our label. The individual prices of entries were left alone, but we removed all entries with price above 350, in order to reduce outliers and because it makes more sense for real life application. The original data set included entries from a large variety of wines, including syrah, moscato, and port. We first went through and labeled a large portion of the data as either RED, WHITE, ROSE, DESSERT, or SHERRY, based on what category the particular variety fit in to. Then we eliminated all but the entries of the RED and WHITE categories, changing the RED label to 0 and the WHITE label to 1. We used 20000 elements for training and 4000 for testing. One notable property of the data is that there was a greater proportion of red wines than there were white (about 2:1), this was carried over to the training and testing.

We were trying to predict the score of a wine based on its price and type. Our hypothesis was that the higher priced red wines would have a better score.

We used linear classification for this task because we are looking for the price point within two different categories where the chance of a particular element being a certain quality becomes likely.

We selected our training data by first organizing the large list of (data that had been pruned as described above) by the arbitrary numbering provided in the original data set. Then we took the first 20000 elements as training data and the next 4000 elements as testing data.

2.
First we used data from the San Francisco bike share program. Each entry in this set represented a bike rental and included the id of the station from where the bike was rented, the duration of the rental, and the id of the station to where the bike was returned. We initially tried to use the start location and the duration to predict if the return location was the same, but there was no discernible pattern for the algorithm to pick up on. Next we tried using the start and end locations to predict the length of the trip, but we ran into similar issues.

The data set we switched to was a set of wine reviews. Each entry represented a bottle of wine, and included the country of production, variety, price, and score given. We created several smaller data sets from this large set, each using score as a label. Except for wine_4, which also included the country of production, each data set only included the score class, price, and type class (0 for red, 1 for white). wine_1 through wine_4 had 2 score classes (0 for low, 1 for high). wine_1 contains a small amount of data which we used to test that we had working code. wine_2 increased the size of the data. wine_3 further increased the size of the data, bringing the training data to 20000 elements and the test data to 4000 elements. wine_4 is the same size as wine_3, but includes the country of production. We found it difficult to properly display this extra information on the chart printout. wine_5 added another score class, giving us 3 total (0 for low, 1 for mid, 2 for high). We found this to be the most useful partition. wine_6 added yet another score class giving us 4 total (0 for low, 1 for mid-low, 2 for mid-high, 3 for high).

4.
We have learned that, within our data, higher priced wines tend to get higher scores, while lower priced wines tend to get lower scores, though there were wines receiving both medium and high scores in all price ranges. There were no wines priced over $180 that received a score lower than 88. As well, there was only one white wine priced above $75 that scored below 88.

5.
The project could be improved by considering more factors, including country of origin. One challenge that we would face in doing that is how to properly display that data. In the time that we looked, we could not find a way to easily display a 3-dimensional graph using DL4J.
